{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc945032-07e7-486e-a0eb-5e79628232fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d822f2fa-6875-46b2-8f22-3e6851f64a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # загружаем open_ai_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff99b1-466b-4764-a082-2b3e7fa8b4ba",
   "metadata": {},
   "source": [
    "## Принципы написания промптов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0409315-3502-45cb-a766-50c5973f6796",
   "metadata": {},
   "source": [
    "* Принцип 1: Писать ясные и точные инструкции\n",
    "* Принцип 2: Дать модели время на \"подумать\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030b32f-5110-4344-8fd8-86adb4009612",
   "metadata": {},
   "source": [
    "### Тактики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5865ee-00aa-47b3-a4f6-08457d05514d",
   "metadata": {},
   "source": [
    "#### Тактика 1: Использовать разделители для разграничения частей того, где инструкция, а где текст.\n",
    "* Разделителями может быть все, что угодно подобно: ```, \"\"\", < >, `<tag> </tag>`, `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1447c3b7-1a0c-4634-9cb4-004b256860f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293b518-b93b-4477-9c8c-d6cd66c4fddd",
   "metadata": {},
   "source": [
    "#### Тактика 2: запрашивать структурированный вывод\n",
    "* JSON, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356fa179-fdc6-4db4-ae4d-3bea02056a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Generate a list of three made-up book titles along \\ \n",
    "with their authors and genres. \n",
    "Provide them in JSON format with the following keys: \n",
    "book_id, title, author, genre.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ada76c-12ec-4809-96e6-cb1597491de4",
   "metadata": {},
   "source": [
    "#### Тактика 3: Просить модель проверить удовлетворено ли условие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85054f79-3ff7-4fe3-880d-d428a039b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some \\ \n",
    "water boiling. While that's happening, \\ \n",
    "grab a cup and put a tea bag in it. Once the water is \\ \n",
    "hot enough, just pour it over the tea bag. \\ \n",
    "Let it sit for a bit so the tea can steep. After a \\ \n",
    "few minutes, take out the tea bag. If you \\ \n",
    "like, you can add some sugar or milk to taste. \\ \n",
    "And that's it! You've got yourself a delicious \\ \n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e444fcdb-dabb-4339-8c06-5de550371d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = f\"\"\"\n",
    "The sun is shining brightly today, and the birds are \\\n",
    "singing. It's a beautiful day to go for a \\ \n",
    "walk in the park. The flowers are blooming, and the \\ \n",
    "trees are swaying gently in the breeze. People \\ \n",
    "are out and about, enjoying the lovely weather. \\ \n",
    "Some are having picnics, while others are playing \\ \n",
    "games or simply relaxing on the grass. It's a \\ \n",
    "perfect day to spend time outdoors and appreciate the \\ \n",
    "beauty of nature.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af89ede-4755-4ac9-9f5a-d05d4025b22c",
   "metadata": {},
   "source": [
    "#### Тактика 4: \"Few-shot\" prompting или написание промпта несколькими кадрами (примерами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae1c154-97db-41a5-85f6-9d5217c685a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest \\ \n",
    "valley flows from a modest spring; the \\ \n",
    "grandest symphony originates from a single note; \\ \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a630458f-3028-415e-87e2-ca1631cd17bc",
   "metadata": {},
   "source": [
    "### Принцип 2: Дать модели время на \"подумать\"\n",
    "\n",
    "#### Тактика 1: Уточнить шаги, которые необходимы для решения задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc45255-4578-4c8d-beb7-7d38d8ef4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "In a charming village, siblings Jack and Jill set out on \\ \n",
    "a quest to fetch water from a hilltop \\ \n",
    "well. As they climbed, singing joyfully, misfortune \\ \n",
    "struck—Jack tripped on a stone and tumbled \\ \n",
    "down the hill, with Jill following suit. \\ \n",
    "Though slightly battered, the pair returned home to \\ \n",
    "comforting embraces. Despite the mishap, \\ \n",
    "their adventurous spirits remained undimmed, and they \\ \n",
    "continued exploring with delight.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple \\\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following \\\n",
    "keys: french_summary, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9194af7-6691-4f13-9811-5567ad42c3e3",
   "metadata": {},
   "source": [
    "**Попросить вывод в определенном формате**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafef964-056b-4d98-9d3f-38282a191e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Summarize the following text delimited by \n",
    "  <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the \n",
    "  following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in Italian summary>\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215bbc7-4b4e-4de3-b2f3-40fbd3aca8a0",
   "metadata": {},
   "source": [
    "#### Тактика 2: Проинструктировать модель выработать свое собственно решение перед тем, как торопиться с выводами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a57ba805-1b85-4ad1-b212-06ee16b4f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\ \n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ad192-145f-4672-ab8f-7f53cd75b7a1",
   "metadata": {},
   "source": [
    "**Обратите внимание, что решение студента не всегда верно.**  \n",
    "**Это можно исправить, проинструктировав модель сначала выработать свое собственное решение.**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df511b7f-0376-4a8d-9038-da061c71d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution \\\n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem. \n",
    "- Then compare your solution to the student's solution \\ \n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\n",
    "working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\n",
    "as a function of the number of square feet.\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cdc47f-70c2-4faa-ae7f-1e1b8e9868fe",
   "metadata": {},
   "source": [
    "## Итеративность разработки промптов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e28023-6ab7-480a-91f6-25b24f0bf8b5",
   "metadata": {},
   "source": [
    "Написали промпт, оценили вывод модели, осознали чего не хватает, откорректировали промпт, повторяем до удовлетворения выводом модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6365117-ffee-4858-aa86-4f88d39375ab",
   "metadata": {},
   "source": [
    "## Саммаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19a3e3-c0fe-4fad-ad0c-c8e43836ca95",
   "metadata": {},
   "source": [
    "Просим модель кратко описать текст, уточняя на что делать акцент и, например, попросить уложиться в 50 слов.  \n",
    "Вместа краткого описания (summary) можно пробовать использовать инструкцию выделить/определить (extract)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325fd5e-b442-40ca-8dc6-0df9312f1e0a",
   "metadata": {},
   "source": [
    "## Inferring или выводы о теме и сентименте"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1bc42d-706a-4ab9-84be-8bc4bab879b0",
   "metadata": {},
   "source": [
    "Сентименты:\n",
    "* Идентифицируй сентимент одним словом (положительный/отрицательный)\n",
    "* Идентифицируй типы эмоций\n",
    "* Идентифицируй конкретную эмоцию, например, гнев."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f29cb-ab97-4858-8d37-2807506015a0",
   "metadata": {},
   "source": [
    "NER:\n",
    "* Выдели товар, о котором идет речь в отзыве\n",
    "* Выдели название компании, которая произвела товар в отзыве"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d8aa0-1ffa-45af-aaff-a28e1bdb3b30",
   "metadata": {},
   "source": [
    "Можно давать сразу несколько инструкций:\n",
    "* Сентимент\n",
    "* Товар\n",
    "* Тема"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04eb07-59df-4574-b7a3-0567bd5654f7",
   "metadata": {},
   "source": [
    "Определение тем:\n",
    "* Определи 5 тем, которые есть в сообщение\n",
    "* Определи, говорится ли в сообщение об одной из каких-то 10 тем\n",
    "* Верни список с 0 и 1 напротив тем, которые есть в каком-то сообщении"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2b09a-2332-4731-bfd1-0e0cac8334e0",
   "metadata": {},
   "source": [
    "## Трансформация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e09f15-6af9-4ca1-a47d-c6523486a9ab",
   "metadata": {},
   "source": [
    "* Переведи текст\n",
    "* Измени тон сообщения (из обычного в бизнес, например) - можно описать тон, или попросить модель описать тон и использовать его позже\n",
    "* Трансформировать JSON в HTML\n",
    "* Проверить на пунктуационные и грамматические ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553196e0-b0e9-43aa-8156-f59e4f1bc603",
   "metadata": {},
   "source": [
    "## Expanding (Расширение)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d8fea-5f11-4556-934b-8fd5199bc4b8",
   "metadata": {},
   "source": [
    "Кастомизировать автоматический ответ к письму клиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d490875-8c6e-4832-b1d2-bc1587fd6ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the sentiment from the lesson on \"inferring\",\n",
    "# and the original customer message, customize the email\n",
    "sentiment = \"negative\"\n",
    "\n",
    "# review for a blender\n",
    "review = f\"\"\"\n",
    "So, they still had the 17 piece system on seasonal \\\n",
    "sale for around $49 in the month of November, about \\\n",
    "half off, but for some reason (call it price gouging) \\\n",
    "around the second week of December the prices all went \\\n",
    "up to about anywhere from between $70-$89 for the same \\\n",
    "system. And the 11 piece system went up around $10 or \\\n",
    "so in price also from the earlier sale price of $29. \\\n",
    "So it looks okay, but if you look at the base, the part \\\n",
    "where the blade locks into place doesn’t look as good \\\n",
    "as in previous editions from a few years ago, but I \\\n",
    "plan to be very gentle with it (example, I crush \\\n",
    "very hard items like beans, ice, rice, etc. in the \\ \n",
    "blender first then pulverize them in the serving size \\\n",
    "I want in the blender then switch to the whipping \\\n",
    "blade for a finer flour, and use the cross cutting blade \\\n",
    "first when making smoothies, then use the flat blade \\\n",
    "if I need them finer/less pulpy). Special tip when making \\\n",
    "smoothies, finely cut and freeze the fruits and \\\n",
    "vegetables (if using spinach-lightly stew soften the \\ \n",
    "spinach then freeze until ready for use-and if making \\\n",
    "sorbet, use a small to medium sized food processor) \\ \n",
    "that you plan to use that way you can avoid adding so \\\n",
    "much ice if at all-when making your smoothie. \\\n",
    "After about a year, the motor was making a funny noise. \\\n",
    "I called customer service but the warranty expired \\\n",
    "already, so I had to buy another one. FYI: The overall \\\n",
    "quality has gone done in these types of products, so \\\n",
    "they are kind of counting on brand recognition and \\\n",
    "consumer loyalty to maintain sales. Got it in about \\\n",
    "two days.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d83676-f248-41bd-a5ef-1d7e80aeb02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a customer service AI assistant.\n",
    "Your task is to send an email reply to a valued customer.\n",
    "Given the customer email delimited by ```, \\\n",
    "Generate a reply to thank the customer for their review.\n",
    "If the sentiment is positive or neutral, thank them for \\\n",
    "their review.\n",
    "If the sentiment is negative, apologize and suggest that \\\n",
    "they can reach out to customer service. \n",
    "Make sure to use specific details from the review.\n",
    "Write in a concise and professional tone.\n",
    "Sign the email as `AI customer agent`.\n",
    "Customer review: ```{review}```\n",
    "Review sentiment: {sentiment}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8240c-8808-4280-827f-1c33fc4a551e",
   "metadata": {},
   "source": [
    "## Чат бот"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b987ac6-6924-4bdf-bfa5-6731e98a6e19",
   "metadata": {},
   "source": [
    "#### OrderBot\r\n",
    "Мы можем автоматизировать сбор пользовательских запросов и ответов ассистента для создания OrderBot. OrderBot будет принимать заказы в пиццерии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da77be0-f2e9-4342-acd9-9909dadda0b1",
   "metadata": {},
   "source": [
    "Секрет в том, чтобы описать контекст и доступные боту варианты в системном сообщении или инструкции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4d21893-6374-477b-a088-1bd1918ae45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [ {'role':'system', 'content':\"\"\"\n",
    "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
    "You first greet the customer, then collects the order, \\\n",
    "and then asks if it's a pickup or delivery. \\\n",
    "You wait to collect the entire order, then summarize it and check for a final \\\n",
    "time if the customer wants to add anything else. \\\n",
    "If it's a delivery, you ask for an address. \\\n",
    "Finally you collect the payment.\\\n",
    "Make sure to clarify all options, extras and sizes to uniquely \\\n",
    "identify the item from the menu.\\\n",
    "You respond in a short, very conversational friendly style. \\\n",
    "The menu includes \\\n",
    "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
    "cheese pizza   10.95, 9.25, 6.50 \\\n",
    "eggplant pizza   11.95, 9.75, 6.75 \\\n",
    "fries 4.50, 3.50 \\\n",
    "greek salad 7.25 \\\n",
    "Toppings: \\\n",
    "extra cheese 2.00, \\\n",
    "mushrooms 1.50 \\\n",
    "sausage 3.00 \\\n",
    "canadian bacon 3.50 \\\n",
    "AI sauce 1.50 \\\n",
    "peppers 1.00 \\\n",
    "Drinks: \\\n",
    "coke 3.00, 2.00, 1.00 \\\n",
    "sprite 3.00, 2.00, 1.00 \\\n",
    "bottled water 5.00 \\\n",
    "\"\"\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012a5dc-093e-4232-b1d5-9114aef68c89",
   "metadata": {},
   "source": [
    "## Примеры работы с LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a7bec-3ba7-4cba-8a5a-8cca5e534f2d",
   "metadata": {},
   "source": [
    "#### Инстациируем чатбота"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d8fb6e6-da5e-4cdc-ba66-146132f11c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "266fc3e9-2389-42c9-8e7d-0e16a791e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "model='gpt-3.5-turbo'\n",
    "# model='gpt-4' # дорогая для использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c75d11a8-6f5a-4a93-87c2-0db9bd02db39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0.0,model=model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c73ed-2345-4fc1-b463-3703a51a0675",
   "metadata": {},
   "source": [
    "#### Как базово работает LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026d5f3-5796-4e4d-92a0-6d54894158b1",
   "metadata": {},
   "source": [
    "![Model I/O](https://python.langchain.com/assets/images/model_io-1f23a36233d7731e93576d6885da2750.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0634c-eaab-47c5-ac69-5b12d0c139ac",
   "metadata": {},
   "source": [
    "1. Создается `promptTemplate` (темнозеленый), который ждет переменных `x` и `y` в этом случае (светлозеленый)\n",
    "2. `promptTemplate` с переменными отправляется модели (фиолетовый)\n",
    "3. Парсится вывод модели при необходимости (синий)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37a446-dbf4-47bc-a5d9-8fcb497ef54d",
   "metadata": {},
   "source": [
    "#### PromptTemplate\n",
    "[Доки питона по промптам](https://python.langchain.com/docs/modules/model_io/prompts/)  \n",
    "[Описание сущности промптов в лангчейне](https://docs.langchain.com/docs/components/prompts/) (Ниже перевод текста по этой ссылке)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ebb1d-3754-4ad8-927c-0314b1cd7d71",
   "metadata": {},
   "source": [
    "Новый способ программирования моделей осуществляется с помощью промптов (prompts). \"Промпт\" относится к вводу для модели. Этот ввод редко жестко закодирован, а чаще всего формируется из нескольких компонентов. Класс PromptTemplate отвечает за построение этого ввода. LangChain предоставляет несколько классов и функций, чтобы упростить создание и работу с промптами.\r\n",
    "\r\n",
    "Этот раздел документации разделен на четыре секции:\r\n",
    "\r\n",
    "PromptValue\r\n",
    "\r\n",
    "Класс, представляющий ввод для модели.\r\n",
    "\r\n",
    "Шаблоны промптов\r\n",
    "\r\n",
    "Класс, отвечающий за построение PromptValue.\r\n",
    "\r\n",
    "Примеры-селекторы\r\n",
    "\r\n",
    "Часто бывает полезно включать примеры в промпты. Эти примеры могут быть закодированы жестко, но часто более эффективно, если они выбираются динамически.\r\n",
    "\r\n",
    "Парсеры вывода\r\n",
    "\r\n",
    "Языковые модели (и модели чатов) выдают текст. Но часто вы можете хотеть получить более структурированную информацию, чем просто текст. Вот где появляются парсеры вывода. Парсеры вывода отвечают за (1) инструктирование модели о том, как должен быть отформатирован вывод, (2) разбор вывода в желаемом формате (включая повторную попытку, если необходимо)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a02bd-bbb3-4ca7-a9a2-acd5ae2aac38",
   "metadata": {},
   "source": [
    "##### Простое построение промпта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "353ed9f7-67a4-4c19-bd34-eb882a368402",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_prompt = '''Переведи текст на русский.\n",
    "\n",
    "Текст: ```{text}```\n",
    "'''\n",
    "# Можно несколько переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74d27105-1bbf-4a3b-8bc9-653086803016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(a_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f2c5b07-67c3-4e73-966f-96e028d97f50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='Переведи текст на русский.\\n\\nТекст: ```{text}```\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01aaf86e-ed86-4b3a-b5b9-86dac21bf6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables\n",
    "# Тогда несколько переменных будут отображены и здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f7c567d-0643-4baa-b572-a9b55c3c5223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eng_text = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad33fbd-b8ba-4b4c-af23-4302add9fb47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# метод перевода промпт в PromptValue\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    text=eng_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1fb48f7-ad7f-41d9-be03-3d08f0b518da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.messages.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "483723e4-8425-4d19-94eb-52e7293adcf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Переведи текст на русский.\\n\\nТекст: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8a62d56-500e-4744-b276-83f6e4033b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adccae2d-1e10-4540-8f42-ae5a62cee21c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аррр, я в ярости, что крышка моего блендера отлетела и облила стенки моей кухни смузи! И чтобы дела стали еще хуже, гарантия не покрывает стоимость уборки моей кухни. Мне нужна твоя помощь прямо сейчас, приятель!\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67339a-9981-4f7d-a124-0000ee4ae2c6",
   "metadata": {},
   "source": [
    "#### OutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49998780-cb08-454a-886b-57227222998d",
   "metadata": {},
   "source": [
    "Хотим, чтобы вывод был в виде JSON.  \n",
    "```JSON\n",
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29df2866-e984-497c-a71d-f57315def54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema # используем для каждого ключа в JSON\n",
    "from langchain.output_parsers import StructuredOutputParser # сам парсер, к которому применяем схемы\n",
    "                                                            # Он то и будет парсить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f515d7e-a115-4143-b871-4a5a2abb412e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Делаем схемы\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "# кладем их в список\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7704669-b9e8-4ff2-bc74-b912334a6cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# создаем парсер\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34f0b5dd-797d-4d97-8a1a-ec38dde0b8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Это один из методов парсера, который возвращает инструкцию для LLM\n",
    "format_instructions = output_parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d898992e-17ca-4b8e-b0c3-de5acb57da6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "917f1b0f-f154-4723-b62a-ab0456273009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Из этого текста будем извлекать JSON\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "792e7316-b34a-485e-90e3-2fc7011cfd74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Создаем инструкцию, в которой есть место для текста и для инструкций форматирования\n",
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "# Создаем из PromptTemplate PromtValue\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "035fd2ec-5ed1-4a58-9263-d8ce805b4f27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим целиком \n",
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9ca05e5-fa51-42d9-9d91-d2e6c2d10edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22ece6e8-85f0-4370-832e-02bc89a359d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": false,\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e0011-6ec3-47e7-91a4-1e6b12849b6e",
   "metadata": {},
   "source": [
    "^ Это строка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ec98663-deec-4d35-ae85-93098a3d2f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(response.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "448f228f-bfa5-43f0-aee1-e85798d1a7e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Переводим ее в JSON с помощью нашего парсера\n",
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f6e180a-1bb3-492e-a6b6-625d8a163f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False,\n",
       " 'delivery_days': '2',\n",
       " 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93dc8ff4-5f87-48ad-bad6-4bd3a68fe43b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95fff2c8-e603-40ad-9a00-2b972d65cd91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6dc5f-57d6-41ea-9eb6-9b5bed219c86",
   "metadata": {},
   "source": [
    "### Память (Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15430d-5bba-40b1-b34f-aee839fa12fe",
   "metadata": {},
   "source": [
    "[Документация](https://python.langchain.com/docs/modules/memory.html)  \n",
    "[Описание сущности](https://docs.langchain.com/docs/components/memory/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f707f-2a7d-4693-8ac5-7115bbea6adc",
   "metadata": {},
   "source": [
    "**Память можно добавить почти куда угодно.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61449148-857f-4bc7-96d2-f26674ac365d",
   "metadata": {},
   "source": [
    "Память - это концепция сохранения и извлечения данных в процессе разговора. Существует два основных метода:\r\n",
    "\r\n",
    "На основе ввода извлекайте все соответствующие фрагменты данных.\r\n",
    "На основе ввода и вывода соответствующим образом обновляйте состояние.\r\n",
    "Существует два основных типа памяти: краткосрочная и долгосрочная.\r\n",
    "\r\n",
    "Краткосрочная память, как правило, относится к тому, как передавать данные в контексте отдельного разговора (как правило, это предыдущие ChatMessages или их резюме).\r\n",
    "\r\n",
    "Долгосрочная память занимается тем, как извлекать и обновлять информацию между разговорами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eded15-f175-41e8-8737-27a38816d9a2",
   "metadata": {},
   "source": [
    "![Memory](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6136c-0727-4d6f-a4af-59064b266b00",
   "metadata": {},
   "source": [
    "##### Типы памяти"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de12a0-0f93-4cf0-bd11-4e5c59d99784",
   "metadata": {},
   "source": [
    "Есть много типов памяти: [Типы памяти](https://python.langchain.com/docs/modules/memory/types/)\n",
    "\n",
    "* ConversationBufferMemory (сохраняет всю историю в буффере)\n",
    "* ConversationBufferWindowMemory (сохраняет последних n сообщений в буффере)\n",
    "* ConversationTokenBufferMemory (сохраняет n токенов в буффере)\n",
    "* ConversationSummaryMemory (сохраняет саммаризацию диалога в буффере)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caa20693-19f2-404a-ae44-ece30d5e7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56a98c29-ca69-4930-9335-cfa36aa0e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=chat, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fb1165d-682e-49c9-9331-7fcd0259e09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Привет!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Привет!', 'history': '', 'response': 'Привет! Как могу помочь?'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation('Привет!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29861b54-beaa-4dd8-8922-e70d38004101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Привет!\n",
      "AI: Привет! Как могу помочь?\n",
      "Human: Я просто проверяю память LangChain\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Я просто проверяю память LangChain',\n",
       " 'history': 'Human: Привет!\\nAI: Привет! Как могу помочь?',\n",
       " 'response': 'О, я знаю о LangChain! Это блокчейн-платформа, разработанная для обеспечения безопасности и прозрачности языковых данных. Она использует технологию блокчейна для хранения и обработки языковых данных, таких как переводы, аудиозаписи и тексты. LangChain также предоставляет возможность создания и управления смарт-контрактами, связанными с языковыми данными. Это очень интересный проект! Что именно вы хотите проверить в памяти LangChain?'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation('Я просто проверяю память LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6316835a-8710-42c6-bfe5-e116e5fd751b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Привет!\n",
      "AI: Привет! Как могу помочь?\n",
      "Human: Я просто проверяю память LangChain\n",
      "AI: О, я знаю о LangChain! Это блокчейн-платформа, разработанная для обеспечения безопасности и прозрачности языковых данных. Она использует технологию блокчейна для хранения и обработки языковых данных, таких как переводы, аудиозаписи и тексты. LangChain также предоставляет возможность создания и управления смарт-контрактами, связанными с языковыми данными. Это очень интересный проект! Что именно вы хотите проверить в памяти LangChain?\n",
      "Human: А вот это называется галлюцинация :)\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'А вот это называется галлюцинация :)',\n",
       " 'history': 'Human: Привет!\\nAI: Привет! Как могу помочь?\\nHuman: Я просто проверяю память LangChain\\nAI: О, я знаю о LangChain! Это блокчейн-платформа, разработанная для обеспечения безопасности и прозрачности языковых данных. Она использует технологию блокчейна для хранения и обработки языковых данных, таких как переводы, аудиозаписи и тексты. LangChain также предоставляет возможность создания и управления смарт-контрактами, связанными с языковыми данными. Это очень интересный проект! Что именно вы хотите проверить в памяти LangChain?',\n",
       " 'response': 'Галлюцинация - это восприятие человеком чего-то, чего на самом деле нет в окружающей действительности. Это может быть зрительное, слуховое, обонятельное или осязательное восприятие, которое не соответствует реальности. Галлюцинации могут быть вызваны различными причинами, такими как наркотики, психические расстройства или физические заболевания.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation('А вот это называется галлюцинация :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fffc95-2f18-4531-abd9-c9fbb443599c",
   "metadata": {},
   "source": [
    "### Цепи (Chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8fb43-42b8-4e67-a4bc-6e6595f572ed",
   "metadata": {},
   "source": [
    "[Доки](https://python.langchain.com/docs/modules/chains.html)  \n",
    "[Описание сущности](https://docs.langchain.com/docs/components/chains/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b47d14-3310-48f5-b4c5-a116f1be7eaa",
   "metadata": {},
   "source": [
    "Цепи - это крайне универсальная концепция, которая представляет собой последовательность модульных компонентов (или других цепей), объединенных определенным образом для достижения общего использования.\r\n",
    "\r\n",
    "Самый часто используемый тип цепи - это LLMChain, который объединяет PromptTemplate, Model и Guardrails, чтобы взять пользовательский ввод, отформатировать его соответствующим образом, передать модели, получить ответ, а затем проверить и исправить (если необходимо) вывод модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b0fa1-511a-4091-b06a-c4f4bb2d88bd",
   "metadata": {},
   "source": [
    "Бывают разные типы цепей:\n",
    "* LLMChain (обычная цепь, промпт с содержанием на вход, ответ на выход)\n",
    "* Sequential Chains (Последовательные цепи) \n",
    "  * SimpleSequentialChain (Простая - что-то на вход, выход первой цепи, становится входом для следующей)\n",
    "  * SequentialChain (Сложная - типа DAG)\n",
    "* Router Chain (Цепь, которая определяет куда пойдет сообщение в зависимости от условий)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c45708-dd65-4f23-b930-218782868f59",
   "metadata": {},
   "source": [
    "**Цепям можно добавить память.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9a93e-403e-4814-84e9-5c7bc288ddc7",
   "metadata": {},
   "source": [
    "#### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d965155-06f9-4ffb-bb1b-db1674398c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c3dc2bb-8b3a-4ddc-8680-0781e30b422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5465c764-076a-4ca2-8238-1ad52e7f73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5633b718-1621-4adb-9bb6-7e107fb02695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f678a44-a95f-4a4f-ab3e-e70133edbac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"RegalRest Linens\"'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520984d-7324-4c63-a8a0-59d0dff75991",
   "metadata": {},
   "source": [
    "#### SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c21c8850-c72c-4a5c-9198-55ef39b9e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "622624a5-0bf3-4120-a13a-e6c68136c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74e4c9ef-16b6-47cb-a296-970ed1c9d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "731851a6-c31b-4f82-9528-f8d2041f7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f47d00d4-2870-4c3f-ae7d-fd5f6af419b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mThe Royal Linens Co.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe Royal Linens Co. specializes in luxury bedding and home textiles, offering exquisite designs and unparalleled comfort for your home.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Royal Linens Co. specializes in luxury bedding and home textiles, offering exquisite designs and unparalleled comfort for your home.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8176a-a70c-4884-a401-55a509aff9bb",
   "metadata": {},
   "source": [
    "#### SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "048cf78a-dec2-43ab-ab44-73f57ecbfce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa5c6556-004e-48c2-86b5-65c5f7381554",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "816b8236-b889-4c25-93b5-7ecfba582594",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c04be3a3-6456-4f47-88a8-582556504e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "092c2498-c531-48ac-a55f-0dd8917ec23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "572b9266-58db-4467-a70f-34d834432a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd3bb41b-1d73-4b57-97cc-c383cc32ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/example_data/Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e12e410f-15ec-4d6d-9f35-50cf104a21af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste poor. The foam doesn't hold, it's weird. I buy the same ones from the store and the taste is much better...\\nOld batch or counterfeit!?\",\n",
       " 'summary': 'The reviewer is dissatisfied with the taste of the product they purchased, suggesting it might be either an old batch or a counterfeit.',\n",
       " 'followup_message': \"Réponse de suivi :\\n\\nCher(e) client(e),\\n\\nMerci d'avoir pris le temps de nous donner votre avis sur notre produit. Nous sommes désolés d'apprendre que vous n'êtes pas satisfait(e) de son goût. Nous tenons à vous assurer que notre entreprise attache une grande importance à la qualité de nos produits.\\n\\nIl est possible que vous ayez reçu un lot périmé ou contrefait. Nous nous excusons sincèrement pour cette expérience désagréable et nous aimerions en savoir plus afin de résoudre ce problème. Pourriez-vous s'il vous plaît nous fournir plus de détails concernant votre achat, tels que la date d'achat, le lieu d'achat et le numéro de lot figurant sur l'emballage ? Cela nous aidera à enquêter sur l'origine du problème.\\n\\nSi cela s'avère être un problème de qualité, nous nous engageons à vous rembourser intégralement ou à vous envoyer un produit de remplacement. Votre satisfaction est notre priorité absolue et nous ferons tout notre possible pour résoudre cette situation.\\n\\nVeuillez nous excuser à nouveau pour les désagréments causés. Nous vous remercions de votre compréhension et de votre coopération.\\n\\nCordialement,\\nL'équipe du service client\"}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b04594-661b-460f-8351-d347251e4a74",
   "metadata": {},
   "source": [
    "#### Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1967194c-728a-4786-b1ce-b1f97cc84f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40c059a0-479b-495b-a6fb-2b0bf2ab35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c38cc9f-1f03-492a-afbc-58c36bd94cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4aff183-ecd9-4adc-9af8-e0a1920df57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "776f3b16-54e7-4399-8f66-0a66ed3b9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e92534e5-6fdb-403b-9919-7a49f55028e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5cd6d19-96fb-4ddb-9fd3-b7c9511874cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee6b4339-255a-4b4b-8286-fe726888c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6e928b7-7052-4947-9979-08b791b57aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4502c7a2-f43b-42df-8068-38ef328a03ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Black body radiation refers to the electromagnetic radiation emitted by an object that absorbs all incident radiation and reflects or transmits none. It is called \"black body\" because it absorbs all wavelengths of light, appearing black at room temperature. \\n\\nAccording to Planck\\'s law, black body radiation is characterized by a continuous spectrum of wavelengths and intensities, which depend on the temperature of the object. As the temperature increases, the peak intensity of the radiation shifts to shorter wavelengths, resulting in a change in color from red to orange, yellow, white, and eventually blue at very high temperatures.\\n\\nBlack body radiation is a fundamental concept in physics and has significant applications in various fields, including astrophysics, thermodynamics, and quantum mechanics. It played a crucial role in the development of quantum theory and understanding the behavior of light and matter.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0d28e2f3-af93-4eae-827d-55dd6adead5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Thank you for your kind words! As a mathematician, I am happy to help with any math questions, no matter how simple or complex they may be.\\n\\nNow, let's solve the problem at hand. The question is asking for the sum of 2 and 2. To find the answer, we can simply add the two numbers together:\\n\\n2 + 2 = 4\\n\\nTherefore, the sum of 2 and 2 is 4.\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f27af963-e0df-49e4-8d3d-701220305480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Every cell in our body contains DNA because DNA is the genetic material that carries the instructions for the development, functioning, and reproduction of all living organisms. DNA contains the information necessary for the synthesis of proteins, which are essential for the structure and function of cells. It serves as a blueprint for the production of specific proteins that determine the characteristics and traits of an organism. Additionally, DNA is responsible for the transmission of genetic information from one generation to the next, ensuring the continuity of life. Therefore, every cell in our body contains DNA to ensure proper cellular function and to pass on genetic information to future generations.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4fd97-8489-472a-947d-9b4db86192f8",
   "metadata": {},
   "source": [
    "### Q&A о документах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58781176-81b3-4c27-b4c4-1a9333ac39ce",
   "metadata": {},
   "source": [
    "**LangChain умеет отвечать на запросы про документы.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1004843-8b48-441c-a3e7-15ccc05ef487",
   "metadata": {},
   "source": [
    "[Доки](https://python.langchain.com/docs/use_cases/question_answering.html)  \n",
    "[Описание сущности](https://docs.langchain.com/docs/use-cases/qa-docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7cd34-ccbb-48e2-9cc8-b69a4783c2f5",
   "metadata": {},
   "source": [
    "Хотя многослойные языковые модели (LLM) могут быть мощными, они не обладают информацией, на которой не были обучены. Если вы хотите использовать LLM для ответов на вопросы о документах, на которых он не был обучен, вам придется предоставить ему информацию о тех документах. Самый распространенный способ сделать это - через \"ретриев-усиленную генерацию\".\r\n",
    "\r\n",
    "Идея ретриев-усиленной генерации заключается в том, что при получении вопроса вы сначала выполняете этап извлечения, чтобы получить все соответствующие документы. Затем вы передаете эти документы вместе с исходным вопросом модели и просите ее сгенерировать ответ. Однако для этого сначала необходимо иметь документы в таком формате, чтобы их можно было опрашивать таким образом. Эта страница описывает общие идеи между этими двумя шагами: (1) введение документов в формат, который можно опрашивать, а затем (2) цепочку ретриев-усиленной генерации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce69f9-84f4-4ac3-b7ef-102deabe417b",
   "metadata": {},
   "source": [
    "### Оценка качества ответов модели (Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4f676-733a-43dc-88ff-44e26581c90f",
   "metadata": {},
   "source": [
    "[Доки](https://python.langchain.com/docs/guides/evaluation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8f70f-2e7e-42cb-a098-d501e7c118b2",
   "metadata": {},
   "source": [
    "В сущности проверять либо глазами, либо самой моделью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967e518-8cf9-4814-88e9-c865bd2df1b5",
   "metadata": {},
   "source": [
    "[String Evaluators](https://python.langchain.com/docs/guides/evaluation/string/): Проверка строк, сравнивая с референтной.  \n",
    "Например: оценка по критерию краткости, или является ли ответ про математику и т.д. См. доки\n",
    "\n",
    "\n",
    "[Trajectory Evaluators](https://python.langchain.com/docs/guides/evaluation/trajectory/): Оценка последовательности действий агента.  \n",
    "\n",
    "\n",
    "[Comparison Evaluators](https://python.langchain.com/docs/guides/evaluation/comparison/): Сравнение выводов моделей, цепей, агентов.  \n",
    "Например, парное сравнение выводов двух разных цепей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90584cc0-405b-4816-9c5d-f0fc9eec15f1",
   "metadata": {},
   "source": [
    "Каждый тип оценщика в LangChain поставляется с готовыми реализациями, которые можно использовать без дополнительных настроек, а также с расширяемым API, позволяющим настраивать их в соответствии с вашими уникальными требованиями. \n",
    "\n",
    "Каждый из этих оценщиков предоставляет простой способ измерить производительность и интегритет моделей на разнообразных данных. Кроме того, расширяемое API позволяет настраивать их поведение в зависимости от ваших специфических требований.\n",
    "\n",
    "Эти оценщики могут быть легко применены в различных сценариях и могут быть использованы с разными цепями и реализациями LLM в библиотеке LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8016fd8-1067-44d1-8388-833dddef8e6e",
   "metadata": {},
   "source": [
    "### Агенты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7dbd3-bd5f-460e-9e6c-92b995ada4c5",
   "metadata": {},
   "source": [
    "[Документация](https://python.langchain.com/docs/modules/agents/)  \n",
    "Концептуальные части переведены здесь.  \n",
    "Можно писать свои тулзы.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f5a32-cd53-4fef-b853-37fbd57aaf9e",
   "metadata": {},
   "source": [
    "Основная идея агентов заключается в использовании LLM для выбора последовательности действий. В цепях последовательность действий закодирована (в коде). В агентах языковая модель используется в качестве рассудочного движка (reasoning engine) для определения, какие действия предпринять и в каком порядке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca64993e-3758-4174-ab83-04a40ab2b684",
   "metadata": {},
   "source": [
    "**Агент (Agent)**  \r\n",
    "Этот класс отвечает за принятие решения о следующем шаге. Это осуществляется с помощью языковой модели и промпта. Этот промпт может включать в себя следующее:\r\n",
    "\r\n",
    "* Личность агента (полезно для того, чтобы он отвечал определенным образом).\r\n",
    "* Контекст агента (полезно, чтобы дать ему больше контекста о типах задач, которые ему предлагаются).\r\n",
    "* Промпт стратегии для более логичного рассуждения (самой известной и широко используемой является ReAct).\r\n",
    "  \r\n",
    "LangChain предоставляет несколько различных типов агентов для начала работы. Тем не менее, вы, вероятно, захотите настроить этих агентов с помощью частей (1) и (2). Для полного списка типов агентов смотрите раздел \"Типы агентов\" ([agent types](https://python.langchain.com/docs/modules/agents/agent_types/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31defe-7c3f-4e4c-a536-f142c025cbb5",
   "metadata": {},
   "source": [
    "**Инструменты (Tools)**  \r\n",
    "Инструменты - это функции, которые вызывает агент. Здесь существуют два важных аспекта:\r\n",
    "\r\n",
    "1. Предоставление агенту доступа к правильным инструментам.\r\n",
    "2. Описание инструментов таким образом, чтобы оно было наиболее полезным для агента.\r\n",
    "\r\n",
    "Без обоих аспектов агент, которого вы пытаетесь создать, не будет работать. Если вы не предоставите агенту доступ к правильному набору инструментов, он никогда не сможет достичь поставленной цели. Если вы не правильно описываете инструменты, агент не будет знать, как их правильно использовать.\r\n",
    "\r\n",
    "LangChain предоставляет широкий набор инструментов для начала работы, но также облегчает определение собственных инструментов (включая настраиваемые описания). Для полного списка инструментов, смотрите [здесь](https://python.langchain.com/docs/modules/agents/tools/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f5e86-48fb-4c70-b91d-8a1b9b2ec3f5",
   "metadata": {},
   "source": [
    "**Набор инструментов (Toolkits)**   \r\n",
    "Часто набор инструментов, к которым агент имеет доступ, более важен, чем отдельный инструмент. Для этого LangChain предоставляет концепцию наборов инструментов - групп инструментов, необходимых для достижения конкретных целей. Обычно в каждом наборе инструментов содержится примерно 3-5 инструментов.\r\n",
    "\r\n",
    "LangChain предоставляет широкий набор наборов инструментов для начала работы. Для полного списка наборов инструментов, смотрите [здесь](https://python.langchain.com/docs/modules/agents/toolkits/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ac7bc38-9daa-479b-9e6e-e42c5dd28818",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0f94e09-3ec7-4ea9-afe9-9ed07aae73b2",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6476412d-3ad2-468c-bc9d-8e140f5669eb",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb623c-4637-496a-99c1-668190f8f1fb",
   "metadata": {},
   "source": [
    "Обратите внимание на тип агента и на тулзы.  \n",
    "Агенту можно добавить память."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fdce52c7-cc57-4358-9425-2553c9c8dd09",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0acb33b-7b11-4563-9df0-47fd5a3877c3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can use the calculator tool to find the answer to this question.\n",
      "\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"25% of 300\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe answer is 75.0.\n",
      "Final Answer: 75.0\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the 25% of 300?', 'output': '75.0'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"What is the 25% of 300?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4bfa9b1a-1fb5-4f43-ba8a-12dfe99fd110",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I can use Wikipedia to find out what book Tom M. Mitchell wrote.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Tom M. Mitchell\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Tom M. Mitchell\n",
      "Summary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n",
      "\n",
      "\n",
      "\n",
      "Page: Tom Mitchell (Australian footballer)\n",
      "Summary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood Football Club in the Australian Football League (AFL). He previously played for the Sydney Swans from 2012 to 2016, and the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league's best and fairest player in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood during that season.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe book that Tom M. Mitchell wrote is \"Machine Learning\".\n",
      "Final Answer: Machine Learning\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "result = agent(question) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019c06c-8c2c-4827-bbaf-41a93a24ab0d",
   "metadata": {},
   "source": [
    "#### Python Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c7197fd-3fa5-4207-9c81-9acf4b4c249d",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "175de7cd-24e8-43ac-b253-e43b06a832bd",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "customer_list = [[\"Harrison\", \"Chase\"], \n",
    "                 [\"Lang\", \"Chain\"],\n",
    "                 [\"Dolly\", \"Too\"],\n",
    "                 [\"Elle\", \"Elem\"], \n",
    "                 [\"Geoff\",\"Fusion\"], \n",
    "                 [\"Trance\",\"Former\"],\n",
    "                 [\"Jen\",\"Ayai\"]\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "be4ea9d4-c88e-4f04-afeb-b196144ce1dc",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting order based on last name and then first name.\n",
      "Action: Python_REPL\n",
      "Action Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe customers have been sorted by last name and then first name.\n",
      "Final Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2e70e-7cf5-4244-a3db-59111efe11dc",
   "metadata": {},
   "source": [
    "#### View detailed outputs of the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b75a94f-04be-498e-bb75-48c4a66112bb",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [6.24s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting order based on last name and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting order based on last name and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 328,\n",
      "      \"completion_tokens\": 112,\n",
      "      \"total_tokens\": 440\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [6.24s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting order based on last name and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Python_REPL] Entering Tool run with input:\n",
      "\u001b[0m\"sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Python_REPL] [3ms] Exiting Tool run with output:\n",
      "\u001b[0m\"\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
      "  \"agent_scratchpad\": \"I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting order based on last name and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\\nObservation: \\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting order based on last name and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\\nObservation: \\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] [3.78s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 445,\n",
      "      \"completion_tokens\": 67,\n",
      "      \"total_tokens\": 512\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] [3.79s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [10.03s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug=True\n",
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") \n",
    "langchain.debug=False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
